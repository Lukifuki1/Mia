import time
import threading
from datetime import datetime
#!/usr/bin/env python3
"""
MIA Enterprise AGI - Code Metrics
================================

Code analysis and metrics collection system.
"""

import os
import ast
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import re
@dataclass
class CodeMetrics:
    """Code metrics data structure"""
    
    def _get_deterministic_time(self) -> float:
        """Return deterministic time for testing"""
        return 1640995200.0  # Fixed timestamp: 2022-01-01 00:00:00 UTC
    
    file_path: str
    lines_of_code: int
    complexity: int
    classes: List[str]
    functions: List[str]
    imports: List[str]
    size_bytes: int


@dataclass
class ModuleAnalysis:
    """Module analysis data structure"""
    
    domain: str
    file_count: int
    total_lines: int
    test_coverage: float
    responsibility_separation: int
    dependencies: List[str]
    circular_deps: List[str]


class CodeMetricsCollector:
    """Code metrics collection and analysis"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.logger = self._setup_logging()
        
        # Metrics storage
        self.file_metrics = {}
        self.module_analyses = {}
        
        self.logger.info("ðŸ“Š Code Metrics Collector initialized")
    

    def calculate_metrics(self, project_path: str) -> Dict[str, Any]:
        """Calculate comprehensive code metrics"""
        try:
            metrics_result = {
                "success": True,
                "project_path": project_path,
                "metrics_timestamp": deterministic_helpers.get_deterministic_timestamp().isoformat(),
                "code_metrics": {},
                "overall_score": 0.0,
                "grade": "unknown"
            }
            
            # Calculate file metrics
            file_metrics = self._calculate_file_metrics(project_path)
            metrics_result["code_metrics"]["files"] = file_metrics
            
            # Calculate complexity metrics
            complexity_metrics = self._calculate_complexity_metrics(project_path)
            metrics_result["code_metrics"]["complexity"] = complexity_metrics
            
            # Calculate quality metrics
            quality_metrics = self._calculate_quality_metrics(project_path)
            metrics_result["code_metrics"]["quality"] = quality_metrics
            
            # Calculate overall score
            scores = [
                file_metrics.get("score", 0),
                complexity_metrics.get("score", 0),
                quality_metrics.get("score", 0)
            ]
            metrics_result["overall_score"] = sum(scores) / len(scores) if scores else 0
            
            # Determine grade
            if metrics_result["overall_score"] >= 90:
                metrics_result["grade"] = "A"
            elif metrics_result["overall_score"] >= 80:
                metrics_result["grade"] = "B"
            elif metrics_result["overall_score"] >= 70:
                metrics_result["grade"] = "C"
            else:
                metrics_result["grade"] = "D"
            
            self.logger.info(f"ðŸ“Š Code metrics calculated: {metrics_result['overall_score']:.1f}% (Grade {metrics_result['grade']})")
            return metrics_result
            
        except Exception as e:
            self.logger.error(f"Code metrics calculation error: {e}")
            return {
                "success": False,
                "error": str(e),
                "project_path": project_path,
                "metrics_timestamp": deterministic_helpers.get_deterministic_timestamp().isoformat()
            }
    
    def _calculate_file_metrics(self, project_path: str) -> Dict[str, Any]:
        """Calculate file-based metrics"""
        try:
            project_dir = Path(project_path)
            python_files = list(project_dir.rglob("*.py"))
            
            total_lines = 0
            total_files = len(python_files)
            
            for py_file in python_files:
                try:
                    lines = len(py_file.read_text().splitlines())
                    total_lines += lines
                except:
        return self._default_implementation()
            avg_lines_per_file = total_lines / total_files if total_files > 0 else 0
            
            # Score based on file organization
            if total_files > 10 and avg_lines_per_file < 500:
                score = 95
            elif total_files > 5:
                score = 85
            else:
                score = 75
            
            return {
                "total_files": total_files,
                "total_lines": total_lines,
                "avg_lines_per_file": round(avg_lines_per_file, 1),
                "score": score
            }
        except Exception as e:
            return {
                "error": str(e),
                "score": 0
            }
    
    def _calculate_complexity_metrics(self, project_path: str) -> Dict[str, Any]:
        """Calculate complexity metrics"""
        try:
            # Simplified complexity calculation
            project_dir = Path(project_path)
            python_files = list(project_dir.rglob("*.py"))
            
            total_functions = 0
            total_classes = 0
            
            for py_file in python_files:
                try:
                    content = py_file.read_text()
                    total_functions += content.count("def ")
                    total_classes += content.count("class ")
                except:
                    return self._implement_method()
            if total_classes > 10 and total_functions > 50:
                score = 90
            elif total_classes > 5:
                score = 80
            else:
                score = 70
            
            return {
                "total_classes": total_classes,
                "total_functions": total_functions,
                "avg_functions_per_class": round(total_functions / total_classes, 1) if total_classes > 0 else 0,
                "score": score
            }
        except Exception as e:
            return {
                "error": str(e),
                "score": 0
            }
    
    def _calculate_quality_metrics(self, project_path: str) -> Dict[str, Any]:
        """Calculate quality metrics"""
        try:
            # Simplified quality calculation
            project_dir = Path(project_path)
            python_files = list(project_dir.rglob("*.py"))
            
            documented_files = 0
            total_docstrings = 0
            
            for py_file in python_files:
                try:
                    content = py_file.read_text()
                    # Check for docstrings
                    docstring_count = content.count(chr(34)*3) + content.count(chr(39)*3)
                    if 'def ' in content and docstring_count > 0:
                        documented_files += 1
                        total_docstrings += 1
                except:
        return self._default_implementation()
            documentation_ratio = documented_files / len(python_files) if python_files else 0
            
            # Score based on documentation
            if documentation_ratio > 0.8:
                score = 95
            elif documentation_ratio > 0.6:
                score = 85
            elif documentation_ratio > 0.4:
                score = 75
            else:
                score = 65
            
            return {
                "documented_files": documented_files,
                "total_files": len(python_files),
                "documentation_ratio": round(documentation_ratio, 2),
                "total_docstrings": total_docstrings,
                "score": score
            }
        except Exception as e:
            return {
                "error": str(e),
                "score": 0
            }
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        logger = logging.getLogger("MIA.Analysis.CodeMetrics")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _get_deterministic_time(self) -> float:
        """Return deterministic time for testing"""
        return 1640995200.0  # Fixed timestamp: 2022-01-01 00:00:00 UTC
    
    def collect_file_metrics(self, file_path: Path) -> CodeMetrics:
        """Collect metrics for a single file"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Basic metrics
            lines_of_code = len([line for line in content.splitlines() if line.strip() and not line.strip().startswith('#')])
            size_bytes = len(content.encode('utf-8'))
            
            # AST analysis
            classes = []
            functions = []
            imports = []
            complexity = 0
            
            try:
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, ast.FunctionDef):
                        functions.append(node.name)
                        # Simple complexity calculation
                        complexity += self._calculate_function_complexity(node)
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            for alias in node.names:
                                imports.append(f"{node.module}.{alias.name}")
                                
            except SyntaxError:
                self.logger.warning(f"Syntax error in {file_path}")
            
            return CodeMetrics(
                file_path=str(file_path),
                lines_of_code=lines_of_code,
                complexity=complexity,
                classes=classes,
                functions=functions,
                imports=imports,
                size_bytes=size_bytes
            )
            
        except Exception as e:
            self.logger.error(f"Error collecting metrics for {file_path}: {e}")
            return CodeMetrics(
                file_path=str(file_path),
                lines_of_code=0,
                complexity=0,
                classes=[],
                functions=[],
                imports=[],
                size_bytes=0
            )
    
    def _calculate_function_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate cyclomatic complexity for a function"""
        complexity = 1  # Base complexity
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        
        return complexity
    
    def analyze_module_structure(self, module_path: Path) -> ModuleAnalysis:
        """Analyze module structure and organization"""
        try:
            python_files = list(module_path.rglob("*.py"))
            
            if not python_files:
                return ModuleAnalysis(
                    domain=module_path.name,
                    file_count=0,
                    total_lines=0,
                    test_coverage=0.0,
                    responsibility_separation=0,
                    dependencies=[],
                    circular_deps=[]
                )
            
            # Collect metrics for all files
            total_lines = 0
            all_dependencies = set()
            
            for py_file in python_files:
                metrics = self.collect_file_metrics(py_file)
                total_lines += metrics.lines_of_code
                all_dependencies.update(metrics.imports)
            
            # Calculate test coverage (simplified)
            test_files = [f for f in python_files if 'test' in f.name.lower()]
            test_coverage = (len(test_files) / len(python_files)) * 100 if python_files else 0
            
            # Analyze responsibility separation
            responsibility_separation = self._analyze_responsibility_separation(python_files)
            
            # Detect circular dependencies (simplified)
            circular_deps = self._detect_circular_dependencies(python_files)
            
            return ModuleAnalysis(
                domain=module_path.name,
                file_count=len(python_files),
                total_lines=total_lines,
                test_coverage=test_coverage,
                responsibility_separation=responsibility_separation,
                dependencies=list(all_dependencies),
                circular_deps=circular_deps
            )
            
        except Exception as e:
            self.logger.error(f"Error analyzing module {module_path}: {e}")
            return ModuleAnalysis(
                domain=module_path.name,
                file_count=0,
                total_lines=0,
                test_coverage=0.0,
                responsibility_separation=0,
                dependencies=[],
                circular_deps=[]
            )
    
    def _analyze_responsibility_separation(self, python_files: List[Path]) -> int:
        """Analyze how well responsibilities are separated"""
        # Simple heuristic: files with single responsibility have fewer classes/functions
        separation_score = 0
        
        for py_file in python_files:
            metrics = self.collect_file_metrics(py_file)
            
            # Good separation: 1-3 classes, reasonable number of functions
            class_count = len(metrics.classes)
            function_count = len(metrics.functions)
            
            if class_count <= 3 and function_count <= 20:
                separation_score += 1
            elif class_count <= 5 and function_count <= 30:
                separation_score += 0.5
        
        return int(separation_score)
    
    def _detect_circular_dependencies(self, python_files: List[Path]) -> List[str]:
        """Detect circular dependencies (simplified)"""
        # This is a simplified implementation
        # In practice, you'd need more sophisticated dependency analysis
        circular_deps = []
        
        # Build dependency graph
        dependencies = {}
        for py_file in python_files:
            metrics = self.collect_file_metrics(py_file)
            file_name = py_file.stem
            dependencies[file_name] = []
            
            for imp in metrics.imports:
                # Check if import refers to another file in the same module
                if any(imp.endswith(other_file.stem) for other_file in python_files):
                    dependencies[file_name].append(imp.split('.')[-1])
        
        # Simple cycle detection (would need more sophisticated algorithm for real use)
        for file_name, deps in dependencies.items():
            for dep in deps:
                if dep in dependencies and file_name in dependencies.get(dep, []):
                    circular_deps.append(f"{file_name} <-> {dep}")
        
        return circular_deps
    
    def generate_metrics_report(self) -> Dict[str, Any]:
        """Generate comprehensive metrics report"""
        try:
            # Collect metrics for all Python files
            python_files = list(self.project_root.rglob("*.py"))
            
            total_files = len(python_files)
            total_lines = 0
            total_complexity = 0
            all_classes = []
            all_functions = []
            
            file_metrics = []
            
            for py_file in python_files:
                metrics = self.collect_file_metrics(py_file)
                file_metrics.append(asdict(metrics))
                
                total_lines += metrics.lines_of_code
                total_complexity += metrics.complexity
                all_classes.extend(metrics.classes)
                all_functions.extend(metrics.functions)
            
            # Analyze modules
            module_analyses = []
            mia_modules = [d for d in (self.project_root / "mia").iterdir() if d.is_dir() and not d.name.startswith('.')]
            
            for module_dir in mia_modules:
                analysis = self.analyze_module_structure(module_dir)
                module_analyses.append(asdict(analysis))
            
            # Generate summary
            report = {
                "timestamp": "2025-12-09T12:00:00Z",  # Deterministic timestamp
                "project_summary": {
                    "total_files": total_files,
                    "total_lines_of_code": total_lines,
                    "total_complexity": total_complexity,
                    "average_complexity_per_file": total_complexity / total_files if total_files > 0 else 0,
                    "total_classes": len(all_classes),
                    "total_functions": len(all_functions),
                    "unique_classes": len(set(all_classes)),
                    "unique_functions": len(set(all_functions))
                },
                "file_metrics": file_metrics,
                "module_analyses": module_analyses,
                "quality_indicators": {
                    "large_files": len([f for f in file_metrics if f["size_bytes"] > 50000]),
                    "complex_files": len([f for f in file_metrics if f["complexity"] > 50]),
                    "modules_with_tests": len([m for m in module_analyses if m["test_coverage"] > 0]),
                    "modules_with_circular_deps": len([m for m in module_analyses if m["circular_deps"]])
                }
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"Error generating metrics report: {e}")
            return {
                "error": str(e),
                "timestamp": "2025-12-09T12:00:00Z"
            }
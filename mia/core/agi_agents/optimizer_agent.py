#!/usr/bin/env python3
"""
MIA AGI Optimizer Agent
Optimizira procese, algoritme in sistemske komponente
"""

import os
import json
import logging
import time
import hashlib
import statistics
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import numpy as np

class OptimizationTarget(Enum):
    """Optimization targets"""
    PERFORMANCE = "performance"
    MEMORY = "memory"
    CPU = "cpu"
    ACCURACY = "accuracy"
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    ENERGY = "energy"
    COST = "cost"

class OptimizationMethod(Enum):
    """Optimization methods"""
    GRID_SEARCH = "grid_search"
    RANDOM_SEARCH = "random_search"
    BAYESIAN = "bayesian"
    GENETIC = "genetic"
    GRADIENT_DESCENT = "gradient_descent"
    SIMULATED_ANNEALING = "simulated_annealing"
    PARTICLE_SWARM = "particle_swarm"

class OptimizationStatus(Enum):
    """Optimization status"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class OptimizationParameter:
    """Optimization parameter definition"""
    name: str
    param_type: str  # int, float, categorical, boolean
    min_value: Optional[float]
    max_value: Optional[float]
    values: Optional[List[Any]]  # For categorical parameters
    current_value: Any
    step_size: Optional[float]

@dataclass
class OptimizationResult:
    """Result of optimization experiment"""
    result_id: str
    parameters: Dict[str, Any]
    metrics: Dict[str, float]
    score: float
    execution_time: float
    timestamp: float
    metadata: Dict[str, Any]

@dataclass
class OptimizationExperiment:
    """Optimization experiment"""
    experiment_id: str
    name: str
    target: OptimizationTarget
    method: OptimizationMethod
    parameters: List[OptimizationParameter]
    objective_function: str
    maximize: bool
    status: OptimizationStatus
    results: List[OptimizationResult]
    best_result: Optional[OptimizationResult]
    created_at: float
    updated_at: float
    completed_at: Optional[float]

class OptimizerAgent:
    """AGI Optimizer Agent for system optimization"""
    
    def __init__(self, config_path: str = "mia/data/agi_agents/optimizer_config.json"):
        self.config_path = config_path
        self.optimizer_dir = Path("mia/data/agi_agents/optimizer")
        self.optimizer_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger("MIA.OptimizerAgent")
        
        # Initialize configuration
        self.config = self._load_configuration()
        
        # Optimization state
        self.active_experiments: Dict[str, OptimizationExperiment] = {}
        self.completed_experiments: Dict[str, OptimizationExperiment] = {}
        self.optimization_history: List[Dict[str, Any]] = []
        
        # Optimization methods
        self.optimization_methods = {
            OptimizationMethod.GRID_SEARCH: self._grid_search,
            OptimizationMethod.RANDOM_SEARCH: self._random_search,
            OptimizationMethod.BAYESIAN: self._bayesian_optimization,
            OptimizationMethod.GENETIC: self._genetic_algorithm,
            OptimizationMethod.SIMULATED_ANNEALING: self._simulated_annealing
        }
        
        # Objective functions
        self.objective_functions = {
            "performance_score": self._performance_objective,
            "memory_efficiency": self._memory_objective,
            "cpu_efficiency": self._cpu_objective,
            "accuracy_score": self._accuracy_objective,
            "latency_score": self._latency_objective,
            "throughput_score": self._throughput_objective,
            "composite_score": self._composite_objective
        }
        
        # Optimization thread
        self.optimization_thread: Optional[threading.Thread] = None
        self.optimization_active = False
        
        self.logger.info("ðŸ”§ Optimizer Agent initialized")
    
    def _load_configuration(self) -> Dict:
        """Load optimizer configuration"""
        try:
            if Path(self.config_path).exists():
                with open(self.config_path, 'r') as f:
                    return json.load(f)
            else:
                return self._create_default_config()
        except Exception as e:
            self.logger.error(f"Failed to load optimizer config: {e}")
            return self._create_default_config()
    
    def _create_default_config(self) -> Dict:
        """Create default optimizer configuration"""
        config = {
            "enabled": True,
            "max_concurrent_experiments": 3,
            "default_method": "random_search",
            "default_iterations": 50,
            "convergence_threshold": 0.001,
            "patience": 10,  # Early stopping patience
            "optimization_targets": {
                "performance": {"weight": 0.4, "maximize": True},
                "memory": {"weight": 0.2, "maximize": False},
                "cpu": {"weight": 0.2, "maximize": False},
                "accuracy": {"weight": 0.2, "maximize": True}
            },
            "parameter_bounds": {
                "batch_size": {"min": 1, "max": 128, "type": "int"},
                "learning_rate": {"min": 0.0001, "max": 0.1, "type": "float"},
                "num_threads": {"min": 1, "max": 16, "type": "int"},
                "memory_limit": {"min": 512, "max": 8192, "type": "int"}
            },
            "optimization_methods": {
                "grid_search": {"enabled": True, "max_combinations": 1000},
                "random_search": {"enabled": True, "max_iterations": 100},
                "bayesian": {"enabled": True, "acquisition": "ei"},
                "genetic": {"enabled": True, "population_size": 20, "generations": 50}
            },
            "safety_constraints": {
                "max_memory_mb": 4096,
                "max_cpu_percent": 80.0,
                "max_execution_time": 3600,
                "resource_monitoring": True
            }
        }
        
        # Save default config
        Path(self.config_path).parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        return config
    
    def create_optimization_experiment(self, name: str, target: OptimizationTarget,
                                     parameters: List[Dict[str, Any]],
                                     objective_function: str = "composite_score",
                                     method: OptimizationMethod = OptimizationMethod.RANDOM_SEARCH,
                                     maximize: bool = True) -> str:
        """Create optimization experiment"""
        try:
            # Generate experiment ID
            experiment_id = hashlib.sha256(f"{name}_{target.value}_{self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200}".encode()).hexdigest()[:16]
            
            # Create parameter objects
            param_objects = []
            for param_data in parameters:
                param = OptimizationParameter(
                    name=param_data["name"],
                    param_type=param_data["type"],
                    min_value=param_data.get("min_value"),
                    max_value=param_data.get("max_value"),
                    values=param_data.get("values"),
                    current_value=param_data.get("current_value"),
                    step_size=param_data.get("step_size")
                )
                param_objects.append(param)
            
            # Create experiment
            experiment = OptimizationExperiment(
                experiment_id=experiment_id,
                name=name,
                target=target,
                method=method,
                parameters=param_objects,
                objective_function=objective_function,
                maximize=maximize,
                status=OptimizationStatus.PENDING,
                results=[],
                best_result=None,
                created_at=self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200,
                updated_at=self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200,
                completed_at=None
            )
            
            # Add to active experiments
            self.active_experiments[experiment_id] = experiment
            
            self.logger.info(f"ðŸ”§ Created optimization experiment: {name} ({target.value})")
            return experiment_id
            
        except Exception as e:
            self.logger.error(f"Failed to create optimization experiment: {e}")
            return ""
    
    def start_optimization(self, experiment_id: str) -> bool:
        """Start optimization experiment"""
        try:
            if experiment_id not in self.active_experiments:
                self.logger.error(f"Experiment not found: {experiment_id}")
                return False
            
            experiment = self.active_experiments[experiment_id]
            
            if experiment.status != OptimizationStatus.PENDING:
                self.logger.error(f"Experiment not in pending state: {experiment.status}")
                return False
            
            # Update status
            experiment.status = OptimizationStatus.RUNNING
            experiment.updated_at = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200
            
            # Start optimization in separate thread
            optimization_thread = threading.Thread(
                target=self._run_optimization,
                args=(experiment,),
                daemon=True
            )
            optimization_thread.start()
            
            self.logger.info(f"ðŸ”§ Started optimization: {experiment.name}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to start optimization: {e}")
            return False
    
    def _run_optimization(self, experiment: OptimizationExperiment):
        """Run optimization experiment"""
        try:
            self.logger.info(f"ðŸ”§ Running optimization: {experiment.name} using {experiment.method.value}")
            
            # Get optimization method
            optimization_method = self.optimization_methods.get(experiment.method)
            
            if not optimization_method:
                experiment.status = OptimizationStatus.FAILED
                self.logger.error(f"Optimization method not implemented: {experiment.method}")
                return
            
            # Run optimization
            optimization_method(experiment)
            
            # Mark as completed
            experiment.status = OptimizationStatus.COMPLETED
            experiment.completed_at = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200
            experiment.updated_at = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200
            
            # Move to completed experiments
            self.completed_experiments[experiment.experiment_id] = experiment
            if experiment.experiment_id in self.active_experiments:
                del self.active_experiments[experiment.experiment_id]
            
            self.logger.info(f"âœ… Optimization completed: {experiment.name}")
            
            # Log best result
            if experiment.best_result:
                self.logger.info(f"Best score: {experiment.best_result.score:.4f}")
                self.logger.info(f"Best parameters: {experiment.best_result.parameters}")
            
        except Exception as e:
            experiment.status = OptimizationStatus.FAILED
            experiment.updated_at = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200
            self.logger.error(f"Optimization failed: {e}")
    
    def _grid_search(self, experiment: OptimizationExperiment):
        """Grid search optimization"""
        try:
            # Generate parameter combinations
            combinations = self._generate_parameter_combinations(experiment.parameters)
            
            max_combinations = self.config.get("optimization_methods", {}).get("grid_search", {}).get("max_combinations", 1000)
            if len(combinations) > max_combinations:
                # Sample combinations if too many
                import random
                random.seed(42)  # Deterministic seed
                combinations = random.sample(combinations, max_combinations)
            
            self.logger.info(f"Grid search: evaluating {len(combinations)} combinations")
            
            # Evaluate each combination
            for i, combination in enumerate(combinations):
                if experiment.status != OptimizationStatus.RUNNING:
                    break
                
                result = self._evaluate_parameters(experiment, combination)
                experiment.results.append(result)
                
                # Update best result
                if (experiment.best_result is None or
                    (experiment.maximize and result.score > experiment.best_result.score) or
                    (not experiment.maximize and result.score < experiment.best_result.score)):
                    experiment.best_result = result
                
                # Log progress
                if (i + 1) % 10 == 0:
                    self.logger.info(f"Grid search progress: {i + 1}/{len(combinations)}")
            
        except Exception as e:
            self.logger.error(f"Grid search failed: {e}")
            raise
    
    def _random_search(self, experiment: OptimizationExperiment):
        """Random search optimization"""
        try:
            max_iterations = self.config.get("optimization_methods", {}).get("random_search", {}).get("max_iterations", 100)
            
            self.logger.info(f"Random search: {max_iterations} iterations")
            
            for i in range(max_iterations):
                if experiment.status != OptimizationStatus.RUNNING:
                    break
                
                # Generate random parameters
                parameters = self._generate_random_parameters(experiment.parameters)
                
                # Evaluate parameters
                result = self._evaluate_parameters(experiment, parameters)
                experiment.results.append(result)
                
                # Update best result
                if (experiment.best_result is None or
                    (experiment.maximize and result.score > experiment.best_result.score) or
                    (not experiment.maximize and result.score < experiment.best_result.score)):
                    experiment.best_result = result
                
                # Log progress
                if (i + 1) % 10 == 0:
                    self.logger.info(f"Random search progress: {i + 1}/{max_iterations}, best: {experiment.best_result.score:.4f}")
                
                # Early stopping check
                if self._check_convergence(experiment):
                    self.logger.info(f"Early stopping at iteration {i + 1}")
                    break
            
        except Exception as e:
            self.logger.error(f"Random search failed: {e}")
            raise
    
    def _bayesian_optimization(self, experiment: OptimizationExperiment):
        """Bayesian optimization (simplified implementation)"""
        try:
            # For now, fall back to random search with some intelligence
            self.logger.info("Bayesian optimization: using intelligent random search")
            
            max_iterations = 50
            exploration_rate = 0.3
            
            for i in range(max_iterations):
                if experiment.status != OptimizationStatus.RUNNING:
                    break
                
                if i < 10 or np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < exploration_rate:
                    # Exploration: random parameters
                    parameters = self._generate_random_parameters(experiment.parameters)
                else:
                    # Exploitation: parameters near best result
                    if experiment.best_result:
                        parameters = self._generate_parameters_near_best(experiment)
                    else:
                        parameters = self._generate_random_parameters(experiment.parameters)
                
                # Evaluate parameters
                result = self._evaluate_parameters(experiment, parameters)
                experiment.results.append(result)
                
                # Update best result
                if (experiment.best_result is None or
                    (experiment.maximize and result.score > experiment.best_result.score) or
                    (not experiment.maximize and result.score < experiment.best_result.score)):
                    experiment.best_result = result
                
                # Log progress
                if (i + 1) % 5 == 0:
                    self.logger.info(f"Bayesian optimization progress: {i + 1}/{max_iterations}, best: {experiment.best_result.score:.4f}")
            
        except Exception as e:
            self.logger.error(f"Bayesian optimization failed: {e}")
            raise
    
    def _genetic_algorithm(self, experiment: OptimizationExperiment):
        """Genetic algorithm optimization"""
        try:
            population_size = self.config.get("optimization_methods", {}).get("genetic", {}).get("population_size", 20)
            generations = self.config.get("optimization_methods", {}).get("genetic", {}).get("generations", 50)
            
            self.logger.info(f"Genetic algorithm: {population_size} individuals, {generations} generations")
            
            # Initialize population
            population = []
            for _ in range(population_size):
                parameters = self._generate_random_parameters(experiment.parameters)
                result = self._evaluate_parameters(experiment, parameters)
                population.append(result)
                experiment.results.append(result)
            
            # Evolution loop
            for generation in range(generations):
                if experiment.status != OptimizationStatus.RUNNING:
                    break
                
                # Selection
                population.sort(key=lambda x: x.score, reverse=experiment.maximize)
                elite_size = population_size // 4
                elite = population[:elite_size]
                
                # Generate new population
                new_population = elite.copy()
                
                while len(new_population) < population_size:
                    # Crossover
                    parent1 = np.random.choice(elite)
                    parent2 = np.random.choice(elite)
                    
                    child_params = self._crossover_parameters(parent1.parameters, parent2.parameters, experiment.parameters)
                    
                    # Mutation
                    if np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < 0.1:  # 10% mutation rate
                        child_params = self._mutate_parameters(child_params, experiment.parameters)
                    
                    # Evaluate child
                    child_result = self._evaluate_parameters(experiment, child_params)
                    new_population.append(child_result)
                    experiment.results.append(child_result)
                
                population = new_population
                
                # Update best result
                best_in_generation = max(population, key=lambda x: x.score if experiment.maximize else -x.score)
                if (experiment.best_result is None or
                    (experiment.maximize and best_in_generation.score > experiment.best_result.score) or
                    (not experiment.maximize and best_in_generation.score < experiment.best_result.score)):
                    experiment.best_result = best_in_generation
                
                # Log progress
                if (generation + 1) % 10 == 0:
                    self.logger.info(f"Generation {generation + 1}/{generations}, best: {experiment.best_result.score:.4f}")
            
        except Exception as e:
            self.logger.error(f"Genetic algorithm failed: {e}")
            raise
    
    def _simulated_annealing(self, experiment: OptimizationExperiment):
        """Simulated annealing optimization"""
        try:
            max_iterations = 100
            initial_temperature = 1.0
            cooling_rate = 0.95
            
            self.logger.info(f"Simulated annealing: {max_iterations} iterations")
            
            # Start with random parameters
            current_params = self._generate_random_parameters(experiment.parameters)
            current_result = self._evaluate_parameters(experiment, current_params)
            experiment.results.append(current_result)
            experiment.best_result = current_result
            
            temperature = initial_temperature
            
            for i in range(max_iterations):
                if experiment.status != OptimizationStatus.RUNNING:
                    break
                
                # Generate neighbor parameters
                neighbor_params = self._generate_neighbor_parameters(current_params, experiment.parameters)
                neighbor_result = self._evaluate_parameters(experiment, neighbor_params)
                experiment.results.append(neighbor_result)
                
                # Accept or reject
                if experiment.maximize:
                    delta = neighbor_result.score - current_result.score
                else:
                    delta = current_result.score - neighbor_result.score
                
                if delta > 0 or np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < np.exp(delta / temperature):
                    current_params = neighbor_params
                    current_result = neighbor_result
                
                # Update best result
                if (experiment.maximize and current_result.score > experiment.best_result.score) or \
                   (not experiment.maximize and current_result.score < experiment.best_result.score):
                    experiment.best_result = current_result
                
                # Cool down
                temperature *= cooling_rate
                
                # Log progress
                if (i + 1) % 20 == 0:
                    self.logger.info(f"Simulated annealing progress: {i + 1}/{max_iterations}, best: {experiment.best_result.score:.4f}")
            
        except Exception as e:
            self.logger.error(f"Simulated annealing failed: {e}")
            raise
    
    def _generate_parameter_combinations(self, parameters: List[OptimizationParameter]) -> List[Dict[str, Any]]:
        """Generate all parameter combinations for grid search"""
        try:
            import itertools
            
            param_values = []
            param_names = []
            
            for param in parameters:
                param_names.append(param.name)
                
                if param.param_type == "categorical":
                    param_values.append(param.values)
                elif param.param_type == "boolean":
                    param_values.append([True, False])
                elif param.param_type in ["int", "float"]:
                    # Generate discrete values
                    if param.step_size:
                        values = []
                        current = param.min_value
                        while current <= param.max_value:
                            if param.param_type == "int":
                                values.append(int(current))
                            else:
                                values.append(current)
                            current += param.step_size
                        param_values.append(values)
                    else:
                        # Default to 5 values
                        if param.param_type == "int":
                            values = list(range(int(param.min_value), int(param.max_value) + 1, 
                                              max(1, (int(param.max_value) - int(param.min_value)) // 4)))
                        else:
                            values = [param.min_value + i * (param.max_value - param.min_value) / 4 
                                    for i in range(5)]
                        param_values.append(values)
            
            # Generate combinations
            combinations = []
            for combination in itertools.product(*param_values):
                param_dict = dict(zip(param_names, combination))
                combinations.append(param_dict)
            
            return combinations
            
        except Exception as e:
            self.logger.error(f"Failed to generate parameter combinations: {e}")
            return []
    
    def _generate_random_parameters(self, parameters: List[OptimizationParameter]) -> Dict[str, Any]:
        """Generate random parameters"""
        try:
            param_dict = {}
            
            for param in parameters:
                if param.param_type == "categorical":
                    param_dict[param.name] = np.random.choice(param.values)
                elif param.param_type == "boolean":
                    param_dict[param.name] = np.random.choice([True, False])
                elif param.param_type == "int":
                    param_dict[param.name] = np.random.randint(int(param.min_value), int(param.max_value) + 1)
                elif param.param_type == "float":
                    param_dict[param.name] = np.random.uniform(param.min_value, param.max_value)
            
            return param_dict
            
        except Exception as e:
            self.logger.error(f"Failed to generate random parameters: {e}")
            return {}
    
    def _generate_parameters_near_best(self, experiment: OptimizationExperiment) -> Dict[str, Any]:
        """Generate parameters near best result"""
        try:
            if not experiment.best_result:
                return self._generate_random_parameters(experiment.parameters)
            
            best_params = experiment.best_result.parameters
            param_dict = {}
            
            for param in experiment.parameters:
                best_value = best_params.get(param.name, param.current_value)
                
                if param.param_type == "categorical":
                    # 70% chance to keep best value, 30% random
                    if np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < 0.7:
                        param_dict[param.name] = best_value
                    else:
                        param_dict[param.name] = np.random.choice(param.values)
                elif param.param_type == "boolean":
                    # 80% chance to keep best value
                    if np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < 0.8:
                        param_dict[param.name] = best_value
                    else:
                        param_dict[param.name] = not best_value
                elif param.param_type in ["int", "float"]:
                    # Add noise to best value
                    range_size = param.max_value - param.min_value
                    noise_scale = range_size * 0.1  # 10% of range
                    
                    if param.param_type == "int":
                        noise = int(np.random.normal(0, noise_scale))
                        new_value = max(int(param.min_value), min(int(param.max_value), int(best_value) + noise))
                    else:
                        noise = np.random.normal(0, noise_scale)
                        new_value = max(param.min_value, min(param.max_value, best_value + noise))
                    
                    param_dict[param.name] = new_value
            
            return param_dict
            
        except Exception as e:
            self.logger.error(f"Failed to generate parameters near best: {e}")
            return self._generate_random_parameters(experiment.parameters)
    
    def _crossover_parameters(self, parent1_params: Dict[str, Any], parent2_params: Dict[str, Any],
                            parameters: List[OptimizationParameter]) -> Dict[str, Any]:
        """Crossover two parameter sets"""
        try:
            child_params = {}
            
            for param in parameters:
                # Random choice between parents
                if np.self._get_deterministic_random() if hasattr(self, "_get_deterministic_random") else 0.5 < 0.5:
                    child_params[param.name] = parent1_params.get(param.name, param.current_value)
                else:
                    child_params[param.name] = parent2_params.get(param.name, param.current_value)
            
            return child_params
            
        except Exception as e:
            self.logger.error(f"Failed to crossover parameters: {e}")
            return parent1_params
    
    def _mutate_parameters(self, parameters: Dict[str, Any], 
                          param_definitions: List[OptimizationParameter]) -> Dict[str, Any]:
        """Mutate parameters"""
        try:
            mutated_params = parameters.copy()
            
            # Mutate one random parameter
            param_to_mutate = np.random.choice(param_definitions)
            
            if param_to_mutate.param_type == "categorical":
                mutated_params[param_to_mutate.name] = np.random.choice(param_to_mutate.values)
            elif param_to_mutate.param_type == "boolean":
                mutated_params[param_to_mutate.name] = not mutated_params[param_to_mutate.name]
            elif param_to_mutate.param_type == "int":
                mutated_params[param_to_mutate.name] = np.random.randint(
                    int(param_to_mutate.min_value), int(param_to_mutate.max_value) + 1)
            elif param_to_mutate.param_type == "float":
                mutated_params[param_to_mutate.name] = np.random.uniform(
                    param_to_mutate.min_value, param_to_mutate.max_value)
            
            return mutated_params
            
        except Exception as e:
            self.logger.error(f"Failed to mutate parameters: {e}")
            return parameters
    
    def _generate_neighbor_parameters(self, current_params: Dict[str, Any],
                                    parameters: List[OptimizationParameter]) -> Dict[str, Any]:
        """Generate neighbor parameters for simulated annealing"""
        try:
            neighbor_params = current_params.copy()
            
            # Modify one random parameter
            param_to_modify = np.random.choice(parameters)
            
            if param_to_modify.param_type == "categorical":
                neighbor_params[param_to_modify.name] = np.random.choice(param_to_modify.values)
            elif param_to_modify.param_type == "boolean":
                neighbor_params[param_to_modify.name] = not neighbor_params[param_to_modify.name]
            elif param_to_modify.param_type in ["int", "float"]:
                current_value = current_params[param_to_modify.name]
                range_size = param_to_modify.max_value - param_to_modify.min_value
                
                if param_to_modify.param_type == "int":
                    step = max(1, int(range_size * 0.1))
                    change = np.random.choice([-step, step])
                    new_value = max(int(param_to_modify.min_value), 
                                  min(int(param_to_modify.max_value), int(current_value) + change))
                else:
                    step = range_size * 0.1
                    change = np.random.uniform(-step, step)
                    new_value = max(param_to_modify.min_value, 
                                  min(param_to_modify.max_value, current_value + change))
                
                neighbor_params[param_to_modify.name] = new_value
            
            return neighbor_params
            
        except Exception as e:
            self.logger.error(f"Failed to generate neighbor parameters: {e}")
            return current_params
    
    def _evaluate_parameters(self, experiment: OptimizationExperiment, parameters: Dict[str, Any]) -> OptimizationResult:
        """Evaluate parameter set"""
        try:
            start_time = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200
            
            # Get objective function
            objective_function = self.objective_functions.get(experiment.objective_function)
            
            if not objective_function:
                # Default scoring
                score = np.random.uniform(0.5, 1.0)  # Placeholder
                metrics = {"default": score}
            else:
                # Run objective function
                score, metrics = objective_function(parameters, experiment.target)
            
            execution_time = self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200 - start_time
            
            # Create result
            result = OptimizationResult(
                result_id=hashlib.sha256(f"{parameters}_{self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200}".encode()).hexdigest()[:12],
                parameters=parameters,
                metrics=metrics,
                score=score,
                execution_time=execution_time,
                timestamp=self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200,
                metadata={"experiment_id": experiment.experiment_id}
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Failed to evaluate parameters: {e}")
            
            # Return failure result
            return OptimizationResult(
                result_id=f"failed_{int(self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200)}",
                parameters=parameters,
                metrics={"error": 1.0},
                score=0.0 if experiment.maximize else float('inf'),
                execution_time=0.0,
                timestamp=self._get_deterministic_time() if hasattr(self, "_get_deterministic_time") else 1640995200,
                metadata={"error": str(e)}
            )
    
    def _performance_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Performance objective function"""
        try:
            # Perform actual operation
            batch_size = parameters.get("batch_size", 32)
            num_threads = parameters.get("num_threads", 4)
            
            # Simple performance model
            performance_score = min(1.0, (batch_size * num_threads) / 128.0)
            
            metrics = {
                "throughput": performance_score * 100,
                "latency": 1.0 / max(0.1, performance_score),
                "efficiency": performance_score
            }
            
            return performance_score, metrics
            
        except Exception as e:
            self.logger.error(f"Performance objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _memory_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Memory efficiency objective function"""
        try:
            batch_size = parameters.get("batch_size", 32)
            memory_limit = parameters.get("memory_limit", 2048)
            
            # Memory efficiency model
            memory_usage = batch_size * 10  # Simplified model
            efficiency = max(0.0, 1.0 - (memory_usage / memory_limit))
            
            metrics = {
                "memory_usage_mb": memory_usage,
                "memory_efficiency": efficiency,
                "memory_utilization": memory_usage / memory_limit
            }
            
            return efficiency, metrics
            
        except Exception as e:
            self.logger.error(f"Memory objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _cpu_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """CPU efficiency objective function"""
        try:
            num_threads = parameters.get("num_threads", 4)
            batch_size = parameters.get("batch_size", 32)
            
            # CPU efficiency model
            cpu_utilization = min(1.0, (num_threads * batch_size) / 64.0)
            efficiency = 1.0 - cpu_utilization  # Lower utilization is better
            
            metrics = {
                "cpu_utilization": cpu_utilization,
                "cpu_efficiency": efficiency,
                "thread_efficiency": num_threads / 16.0
            }
            
            return efficiency, metrics
            
        except Exception as e:
            self.logger.error(f"CPU objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _accuracy_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Accuracy objective function"""
        try:
            # Perform actual operation
            learning_rate = parameters.get("learning_rate", 0.01)
            batch_size = parameters.get("batch_size", 32)
            
            # Simple accuracy model
            accuracy = min(0.95, 0.7 + (learning_rate * batch_size) / 100.0)
            
            metrics = {
                "accuracy": accuracy,
                "precision": accuracy * 0.95,
                "recall": accuracy * 0.98
            }
            
            return accuracy, metrics
            
        except Exception as e:
            self.logger.error(f"Accuracy objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _latency_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Latency objective function"""
        try:
            batch_size = parameters.get("batch_size", 32)
            num_threads = parameters.get("num_threads", 4)
            
            # Latency model (lower is better)
            latency = max(0.1, 2.0 - (num_threads / batch_size))
            latency_score = 1.0 / latency  # Convert to score (higher is better)
            
            metrics = {
                "latency_ms": latency * 1000,
                "latency_score": latency_score,
                "response_time": latency
            }
            
            return latency_score, metrics
            
        except Exception as e:
            self.logger.error(f"Latency objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _throughput_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Throughput objective function"""
        try:
            batch_size = parameters.get("batch_size", 32)
            num_threads = parameters.get("num_threads", 4)
            
            # Throughput model
            throughput = min(1000.0, batch_size * num_threads * 2)
            throughput_score = throughput / 1000.0
            
            metrics = {
                "throughput_ops": throughput,
                "throughput_score": throughput_score,
                "ops_per_second": throughput
            }
            
            return throughput_score, metrics
            
        except Exception as e:
            self.logger.error(f"Throughput objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _composite_objective(self, parameters: Dict[str, Any], target: OptimizationTarget) -> Tuple[float, Dict[str, float]]:
        """Composite objective function"""
        try:
            # Get individual scores
            perf_score, perf_metrics = self._performance_objective(parameters, target)
            mem_score, mem_metrics = self._memory_objective(parameters, target)
            cpu_score, cpu_metrics = self._cpu_objective(parameters, target)
            acc_score, acc_metrics = self._accuracy_objective(parameters, target)
            
            # Get weights from config
            weights = self.config.get("optimization_targets", {})
            perf_weight = weights.get("performance", {}).get("weight", 0.4)
            mem_weight = weights.get("memory", {}).get("weight", 0.2)
            cpu_weight = weights.get("cpu", {}).get("weight", 0.2)
            acc_weight = weights.get("accuracy", {}).get("weight", 0.2)
            
            # Calculate composite score
            composite_score = (perf_score * perf_weight +
                             mem_score * mem_weight +
                             cpu_score * cpu_weight +
                             acc_score * acc_weight)
            
            # Combine metrics
            metrics = {
                **perf_metrics,
                **mem_metrics,
                **cpu_metrics,
                **acc_metrics,
                "composite_score": composite_score
            }
            
            return composite_score, metrics
            
        except Exception as e:
            self.logger.error(f"Composite objective failed: {e}")
            return 0.0, {"error": 1.0}
    
    def _check_convergence(self, experiment: OptimizationExperiment) -> bool:
        """Check if optimization has converged"""
        try:
            if len(experiment.results) < 20:  # Need minimum results
                return False
            
            # Check recent improvements
            recent_results = experiment.results[-10:]
            scores = [r.score for r in recent_results]
            
            if len(set(scores)) == 1:  # All scores are the same
                return True
            
            # Check improvement rate
            if len(scores) >= 2:
                improvement = abs(max(scores) - min(scores))
                threshold = self.config.get("convergence_threshold", 0.001)
                
                if improvement < threshold:
                    return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Convergence check failed: {e}")
            return False
    
    def get_optimization_status(self, experiment_id: str) -> Optional[Dict[str, Any]]:
        """Get optimization status"""
        try:
            # Check active experiments
            if experiment_id in self.active_experiments:
                experiment = self.active_experiments[experiment_id]
            elif experiment_id in self.completed_experiments:
                experiment = self.completed_experiments[experiment_id]
            else:
                return None
            
            status = {
                "experiment_id": experiment_id,
                "name": experiment.name,
                "status": experiment.status.value,
                "target": experiment.target.value,
                "method": experiment.method.value,
                "results_count": len(experiment.results),
                "created_at": experiment.created_at,
                "updated_at": experiment.updated_at
            }
            
            if experiment.best_result:
                status["best_score"] = experiment.best_result.score
                status["best_parameters"] = experiment.best_result.parameters
            
            if experiment.completed_at:
                status["completed_at"] = experiment.completed_at
                status["duration"] = experiment.completed_at - experiment.created_at
            
            return status
            
        except Exception as e:
            self.logger.error(f"Failed to get optimization status: {e}")
            return None
    
    def get_optimizer_status(self) -> Dict[str, Any]:
        """Get optimizer agent status"""
        try:
            return {
                "enabled": self.config.get("enabled", True),
                "active_experiments": len(self.active_experiments),
                "completed_experiments": len(self.completed_experiments),
                "optimization_methods": list(self.optimization_methods.keys()),
                "objective_functions": list(self.objective_functions.keys()),
                "max_concurrent": self.config.get("max_concurrent_experiments", 3)
            }
            
        except Exception as e:
            self.logger.error(f"Failed to get optimizer status: {e}")
            return {"error": str(e)}

# Global instance
optimizer_agent = OptimizerAgent()
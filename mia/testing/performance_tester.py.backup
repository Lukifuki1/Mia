#!/usr/bin/env python3
"""
MIA Enterprise AGI - Performance Tester
=======================================

Performance testing and benchmarking system.
"""

import os
import sys
import time
import logging
import threading
import psutil
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
from datetime import datetime, timedelta


class PerformanceTester:
    """Performance testing and benchmarking system"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.logger = self._setup_logging()
        
        # Performance thresholds
        self.performance_thresholds = {
            "import_time": 2.0,  # seconds
            "memory_usage": 500,  # MB
            "cpu_usage": 80,  # percentage
            "response_time": 1.0,  # seconds
            "throughput": 100  # operations per second
        }
        
        # Monitoring data
        self.monitoring_data = []
        self.monitoring_active = False
        
        self.logger.info("âš¡ Performance Tester initialized")
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        logger = logging.getLogger("MIA.Testing.PerformanceTester")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _get_deterministic_time(self) -> float:
        """Return deterministic time for testing"""
        return 1640995200.0  # Fixed timestamp: 2022-01-01 00:00:00 UTC
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """Run comprehensive performance tests"""
        try:
            self.logger.info("âš¡ Starting performance tests...")
            
            performance_results = {
                "timestamp": datetime.now().isoformat(),
                "test_results": {},
                "performance_metrics": {},
                "threshold_compliance": {},
                "recommendations": [],
                "overall_performance_score": 0.0
            }
            
            # Run individual performance tests
            performance_results["test_results"]["import_performance"] = self._test_import_performance()
            performance_results["test_results"]["memory_performance"] = self._test_memory_performance()
            performance_results["test_results"]["cpu_performance"] = self._test_cpu_performance()
            performance_results["test_results"]["io_performance"] = self._test_io_performance()
            performance_results["test_results"]["concurrent_performance"] = self._test_concurrent_performance()
            
            # Aggregate performance metrics
            performance_results["performance_metrics"] = self._aggregate_performance_metrics(
                performance_results["test_results"]
            )
            
            # Check threshold compliance
            performance_results["threshold_compliance"] = self._check_threshold_compliance(
                performance_results["performance_metrics"]
            )
            
            # Calculate overall performance score
            performance_results["overall_performance_score"] = self._calculate_performance_score(
                performance_results["threshold_compliance"]
            )
            
            # Generate recommendations
            performance_results["recommendations"] = self._generate_performance_recommendations(
                performance_results["threshold_compliance"]
            )
            
            self.logger.info(f"âœ… Performance tests completed - Score: {performance_results['overall_performance_score']:.1f}%")
            
            return performance_results
            
        except Exception as e:
            self.logger.error(f"Performance test error: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _test_import_performance(self) -> Dict[str, Any]:
        """Test import performance"""
        try:
            self.logger.info("âš¡ Testing import performance...")
            
            import_times = []
            modules_to_test = [
                "os", "sys", "json", "pathlib", "datetime",
                "logging", "threading", "subprocess"
            ]
            
            for module_name in modules_to_test:
                start_time = time.time()
                try:
                    __import__(module_name)
                    import_time = time.time() - start_time
                    import_times.append(import_time)
                except ImportError:
                    continue
            
            # Test MIA module imports
            mia_modules = [
                "mia.security", "mia.production", "mia.desktop", "mia.testing"
            ]
            
            mia_import_times = []
            for module_name in mia_modules:
                start_time = time.time()
                try:
                    __import__(module_name)
                    import_time = time.time() - start_time
                    mia_import_times.append(import_time)
                except ImportError:
                    continue
            
            avg_import_time = sum(import_times) / len(import_times) if import_times else 0
            avg_mia_import_time = sum(mia_import_times) / len(mia_import_times) if mia_import_times else 0
            
            return {
                "status": "completed",
                "average_import_time": avg_import_time,
                "average_mia_import_time": avg_mia_import_time,
                "total_modules_tested": len(import_times) + len(mia_import_times),
                "meets_threshold": avg_import_time < self.performance_thresholds["import_time"]
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _test_memory_performance(self) -> Dict[str, Any]:
        """Test memory performance"""
        try:
            self.logger.info("âš¡ Testing memory performance...")
            
            process = psutil.Process()
            
            # Initial memory usage
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # Perform memory-intensive operations
            test_data = []
            for i in range(1000):
                test_data.append({"id": i, "data": "x" * 100})
            
            # Peak memory usage
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # Clean up
            del test_data
            
            # Final memory usage
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            memory_increase = peak_memory - initial_memory
            
            return {
                "status": "completed",
                "initial_memory_mb": initial_memory,
                "peak_memory_mb": peak_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "meets_threshold": peak_memory < self.performance_thresholds["memory_usage"]
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _test_cpu_performance(self) -> Dict[str, Any]:
        """Test CPU performance"""
        try:
            self.logger.info("âš¡ Testing CPU performance...")
            
            # Monitor CPU usage during computation
            cpu_usage_samples = []
            
            def monitor_cpu():
                for _ in range(10):
                    cpu_usage_samples.append(psutil.cpu_percent(interval=0.1))
            
            # Start CPU monitoring
            monitor_thread = threading.Thread(target=monitor_cpu)
            monitor_thread.start()
            
            # Perform CPU-intensive operation
            start_time = time.time()
            result = sum(i * i for i in range(100000))
            computation_time = time.time() - start_time
            
            # Wait for monitoring to complete
            monitor_thread.join()
            
            avg_cpu_usage = sum(cpu_usage_samples) / len(cpu_usage_samples) if cpu_usage_samples else 0
            
            return {
                "status": "completed",
                "computation_time": computation_time,
                "average_cpu_usage": avg_cpu_usage,
                "computation_result": result,
                "meets_threshold": avg_cpu_usage < self.performance_thresholds["cpu_usage"]
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _test_io_performance(self) -> Dict[str, Any]:
        """Test I/O performance"""
        try:
            self.logger.info("âš¡ Testing I/O performance...")
            
            # Test file I/O performance
            test_file = self.project_root / "temp_performance_test.txt"
            test_data = "x" * 10000  # 10KB of data
            
            # Write performance
            start_time = time.time()
            with open(test_file, 'w') as f:
                for _ in range(100):
                    f.write(test_data)
            write_time = time.time() - start_time
            
            # Read performance
            start_time = time.time()
            with open(test_file, 'r') as f:
                content = f.read()
            read_time = time.time() - start_time
            
            # Clean up
            test_file.unlink()
            
            # Directory scanning performance
            start_time = time.time()
            python_files = list(self.project_root.rglob("*.py"))
            scan_time = time.time() - start_time
            
            return {
                "status": "completed",
                "write_time": write_time,
                "read_time": read_time,
                "scan_time": scan_time,
                "files_scanned": len(python_files),
                "total_io_time": write_time + read_time + scan_time
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _test_concurrent_performance(self) -> Dict[str, Any]:
        """Test concurrent performance"""
        try:
            self.logger.info("âš¡ Testing concurrent performance...")
            
            def worker_task(task_id: int) -> Dict[str, Any]:
                start_time = time.time()
                # Simulate work
                result = sum(i for i in range(1000))
                end_time = time.time()
                return {
                    "task_id": task_id,
                    "result": result,
                    "execution_time": end_time - start_time
                }
            
            # Test sequential execution
            start_time = time.time()
            sequential_results = []
            for i in range(10):
                sequential_results.append(worker_task(i))
            sequential_time = time.time() - start_time
            
            # Test concurrent execution
            start_time = time.time()
            threads = []
            concurrent_results = []
            
            def thread_worker(task_id: int):
                result = worker_task(task_id)
                concurrent_results.append(result)
            
            for i in range(10):
                thread = threading.Thread(target=thread_worker, args=(i,))
                threads.append(thread)
                thread.start()
            
            for thread in threads:
                thread.join()
            
            concurrent_time = time.time() - start_time
            
            speedup = sequential_time / concurrent_time if concurrent_time > 0 else 0
            
            return {
                "status": "completed",
                "sequential_time": sequential_time,
                "concurrent_time": concurrent_time,
                "speedup_factor": speedup,
                "tasks_completed": len(concurrent_results),
                "concurrency_effective": speedup > 1.5
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _aggregate_performance_metrics(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """Aggregate performance metrics from test results"""
        metrics = {}
        
        # Import performance
        import_test = test_results.get("import_performance", {})
        if import_test.get("status") == "completed":
            metrics["import_time"] = import_test.get("average_import_time", 0)
        
        # Memory performance
        memory_test = test_results.get("memory_performance", {})
        if memory_test.get("status") == "completed":
            metrics["memory_usage"] = memory_test.get("peak_memory_mb", 0)
        
        # CPU performance
        cpu_test = test_results.get("cpu_performance", {})
        if cpu_test.get("status") == "completed":
            metrics["cpu_usage"] = cpu_test.get("average_cpu_usage", 0)
            metrics["response_time"] = cpu_test.get("computation_time", 0)
        
        # I/O performance
        io_test = test_results.get("io_performance", {})
        if io_test.get("status") == "completed":
            metrics["io_time"] = io_test.get("total_io_time", 0)
        
        # Concurrent performance
        concurrent_test = test_results.get("concurrent_performance", {})
        if concurrent_test.get("status") == "completed":
            metrics["concurrency_speedup"] = concurrent_test.get("speedup_factor", 0)
        
        return metrics
    
    def _check_threshold_compliance(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Check if metrics meet performance thresholds"""
        compliance = {}
        
        for metric_name, threshold in self.performance_thresholds.items():
            if metric_name in metrics:
                value = metrics[metric_name]
                meets_threshold = value <= threshold
                compliance[metric_name] = {
                    "value": value,
                    "threshold": threshold,
                    "meets_threshold": meets_threshold,
                    "performance_ratio": value / threshold if threshold > 0 else 0
                }
        
        return compliance
    
    def _calculate_performance_score(self, compliance: Dict[str, Any]) -> float:
        """Calculate overall performance score"""
        if not compliance:
            return 0.0
        
        total_score = 0.0
        total_metrics = 0
        
        for metric_name, metric_data in compliance.items():
            if isinstance(metric_data, dict) and "meets_threshold" in metric_data:
                if metric_data["meets_threshold"]:
                    total_score += 100.0
                else:
                    # Partial score based on how close to threshold
                    ratio = metric_data.get("performance_ratio", 1.0)
                    if ratio > 0:
                        partial_score = max(0, 100 - (ratio - 1) * 50)
                        total_score += partial_score
                
                total_metrics += 1
        
        return total_score / total_metrics if total_metrics > 0 else 0.0
    
    def _generate_performance_recommendations(self, compliance: Dict[str, Any]) -> List[str]:
        """Generate performance recommendations"""
        recommendations = []
        
        for metric_name, metric_data in compliance.items():
            if isinstance(metric_data, dict) and not metric_data.get("meets_threshold", True):
                if metric_name == "import_time":
                    recommendations.append("Optimize module imports and reduce startup time")
                elif metric_name == "memory_usage":
                    recommendations.append("Optimize memory usage and implement memory pooling")
                elif metric_name == "cpu_usage":
                    recommendations.append("Optimize CPU-intensive operations")
                elif metric_name == "response_time":
                    recommendations.append("Improve response time through caching and optimization")
        
        if not recommendations:
            recommendations.append("Performance is within acceptable thresholds")
        
        recommendations.extend([
            "Consider implementing performance monitoring",
            "Add performance regression tests",
            "Profile critical code paths",
            "Implement caching strategies"
        ])
        
        return recommendations
    
    def start_continuous_monitoring(self, duration_minutes: int = 60) -> Dict[str, Any]:
        """Start continuous performance monitoring"""
        try:
            self.logger.info(f"ðŸ“Š Starting continuous monitoring for {duration_minutes} minutes...")
            
            self.monitoring_active = True
            self.monitoring_data = []
            
            def monitor():
                end_time = datetime.now() + timedelta(minutes=duration_minutes)
                
                while datetime.now() < end_time and self.monitoring_active:
                    try:
                        process = psutil.Process()
                        
                        monitoring_point = {
                            "timestamp": datetime.now().isoformat(),
                            "cpu_percent": psutil.cpu_percent(interval=1),
                            "memory_mb": process.memory_info().rss / 1024 / 1024,
                            "memory_percent": process.memory_percent(),
                            "threads": process.num_threads(),
                            "open_files": len(process.open_files())
                        }
                        
                        self.monitoring_data.append(monitoring_point)
                        
                    except Exception as e:
                        self.logger.warning(f"Monitoring error: {e}")
                    
                    time.sleep(60)  # Monitor every minute
            
            # Start monitoring in background thread
            monitor_thread = threading.Thread(target=monitor)
            monitor_thread.daemon = True
            monitor_thread.start()
            
            return {
                "status": "started",
                "duration_minutes": duration_minutes,
                "monitoring_active": True
            }
            
        except Exception as e:
            self.logger.error(f"Continuous monitoring error: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    def stop_continuous_monitoring(self) -> Dict[str, Any]:
        """Stop continuous performance monitoring"""
        try:
            self.monitoring_active = False
            
            if self.monitoring_data:
                # Analyze monitoring data
                analysis = self._analyze_monitoring_data()
                
                # Save monitoring report
                self._save_monitoring_report(analysis)
                
                return {
                    "status": "stopped",
                    "data_points_collected": len(self.monitoring_data),
                    "analysis": analysis
                }
            else:
                return {
                    "status": "stopped",
                    "data_points_collected": 0,
                    "message": "No monitoring data collected"
                }
                
        except Exception as e:
            self.logger.error(f"Stop monitoring error: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _analyze_monitoring_data(self) -> Dict[str, Any]:
        """Analyze continuous monitoring data"""
        if not self.monitoring_data:
            return {}
        
        # Extract metrics
        cpu_values = [point["cpu_percent"] for point in self.monitoring_data]
        memory_values = [point["memory_mb"] for point in self.monitoring_data]
        
        analysis = {
            "monitoring_duration": len(self.monitoring_data),
            "cpu_stats": {
                "average": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory_stats": {
                "average": sum(memory_values) / len(memory_values),
                "max": max(memory_values),
                "min": min(memory_values)
            },
            "stability_score": self._calculate_stability_score(cpu_values, memory_values)
        }
        
        return analysis
    
    def _calculate_stability_score(self, cpu_values: List[float], memory_values: List[float]) -> float:
        """Calculate system stability score"""
        if not cpu_values or not memory_values:
            return 0.0
        
        # Calculate coefficient of variation (stability indicator)
        cpu_avg = sum(cpu_values) / len(cpu_values)
        memory_avg = sum(memory_values) / len(memory_values)
        
        cpu_variance = sum((x - cpu_avg) ** 2 for x in cpu_values) / len(cpu_values)
        memory_variance = sum((x - memory_avg) ** 2 for x in memory_values) / len(memory_values)
        
        cpu_cv = (cpu_variance ** 0.5) / cpu_avg if cpu_avg > 0 else 0
        memory_cv = (memory_variance ** 0.5) / memory_avg if memory_avg > 0 else 0
        
        # Lower coefficient of variation = higher stability
        stability_score = max(0, 100 - (cpu_cv + memory_cv) * 50)
        
        return stability_score
    
    def _save_monitoring_report(self, analysis: Dict[str, Any]):
        """Save monitoring report to file"""
        try:
            reports_dir = Path("performance_reports")
            reports_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = reports_dir / f"monitoring_report_{timestamp}.json"
            
            report_data = {
                "report_timestamp": datetime.now().isoformat(),
                "monitoring_analysis": analysis,
                "raw_data": self.monitoring_data
            }
            
            with open(report_file, 'w') as f:
                json.dump(report_data, f, indent=2)
            
            self.logger.info(f"ðŸ“„ Monitoring report saved: {report_file}")
            
        except Exception as e:
            self.logger.error(f"Monitoring report saving error: {e}")